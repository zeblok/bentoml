{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with BentoML\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open-source framework for machine learning **model serving**, aiming to **bridge the gap between Data Science and DevOps**.\n",
    "\n",
    "Data Scientists can easily package their models trained with any ML framework using BentoMl and reproduce the model for serving in production. BentoML helps with managing packaged models in the BentoML format, and allows DevOps to deploy them as online API serving endpoints or offline batch inference jobs, on any cloud platform.\n",
    "\n",
    "This getting started guide demonstrates how to use BentoML to serve a sklearn modeld via a REST API server, and then containerize the model server for production deployment.\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=guides&ea=bentoml-quick-start-guide&dt=bentoml-quick-start-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let's prepare a trained model for serving with BentoML. Train a classifier model on the [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Prediction Service with BentoML\n",
    "\n",
    "Model serving with BentoML comes after a model is trained. The first step is creating a\n",
    "prediction service class, which defines the models required and the inference APIs which\n",
    "contains the serving logic. Here is a minimal prediction service created for serving\n",
    "the iris classifier model trained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris_classifier.py\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "\n",
    "@env(infer_pip_packages=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "    \"\"\"\n",
    "    A minimum prediction service exposing a Scikit-learn model\n",
    "    \"\"\"\n",
    "\n",
    "    @api(input=DataframeInput(), batch=True)\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        An inference API named `predict` with Dataframe input adapter, which codifies\n",
    "        how HTTP requests or CSV files are converted to a pandas Dataframe object as the\n",
    "        inference API function input\n",
    "        \"\"\"\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a prediction service that packages a scikit-learn model and provides\n",
    "an inference API that expects a `pandas.Dataframe` object as its input. BentoML also supports other API input \n",
    "data types including `JsonInput`, `ImageInput`, `FileInput` and \n",
    "[more](https://docs.bentoml.org/en/latest/api/adapters.html).\n",
    "\n",
    "\n",
    "In BentoML, **all inference APIs are suppose to accept a list of inputs and return a \n",
    "list of results**. In the case of `DataframeInput`, each row of the dataframe is mapping\n",
    "to one prediction request received from the client. BentoML will convert HTTP JSON \n",
    "requests into :code:`pandas.DataFrame` object before passing it to the user-defined \n",
    "inference API function.\n",
    " \n",
    "This design allows BentoML to group API requests into small batches while serving online\n",
    "traffic. Comparing to a regular flask or FastAPI based model server, this can increases\n",
    "the overall throughput of the API server by 10-100x depending on the workload.\n",
    "\n",
    "The following code packages the trained model with the prediction service class\n",
    "`IrisClassifier` defined above, and then saves the IrisClassifier instance to disk \n",
    "in the BentoML format for distribution and deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the IrisClassifier class defined above\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', clf)\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML stores all packaged model files under the\n",
    "`~/bentoml/{service_name}/{service_version}` directory by default.\n",
    "The BentoML file format contains all the code, files, and configs required to \n",
    "deploy the model for serving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API Model Serving\n",
    "\n",
    "### To use this REST API from public endpoint use ngrok(see the ngrok part of the notebook) \n",
    "\n",
    "To start a REST API model server with the `IrisClassifier` saved above, use \n",
    "the `bentoml serve` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml serve IrisClassifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to access the API's from outside the Notebook, you can start the dev server with `--run-with-ngrok` option, to gain acccess to the API endpoint via a public endpoint managed by [ngrok](https://ngrok.com/): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip && unzip ngrok-stable-linux-amd64.zip && chmod +x ngrok && sudo mv ngrok  /usr/local/bin && ngrok authtoken <token> && rm ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bentoml serve IrisClassifier:latest --run-with-ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `IrisClassifier` model is now served at `localhost:5000`. Use `curl` command to send\n",
    "a prediction request:\n",
    "\n",
    "```bash\n",
    "curl -i \\\n",
    "--header \"Content-Type: application/json\" \\\n",
    "--request POST \\\n",
    "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "localhost:5000/predict\n",
    "```\n",
    "\n",
    "Or with `python` and [request library](https://requests.readthedocs.io/):\n",
    "```python\n",
    "import requests\n",
    "response = requests.post(\"http://127.0.0.1:5000/predict\", json=[[5.1, 3.5, 1.4, 0.2]])\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "Note that BentoML API server automatically converts the Dataframe JSON format into a\n",
    "`pandas.DataFrame` object before sending it to the user-defined inference API function.\n",
    "\n",
    "The BentoML API server also provides a simple web UI dashboard.\n",
    "Go to http://localhost:5000 in the browser and use the Web UI to send\n",
    "prediction request:\n",
    "\n",
    "![BentoML API Server Web UI Screenshot](https://raw.githubusercontent.com/bentoml/BentoML/master/guides/quick-start/bento-api-server-web-ui.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
