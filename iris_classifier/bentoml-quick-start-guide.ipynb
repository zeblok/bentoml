{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with BentoML\n",
    "\n",
    "[BentoML](http://bentoml.ai) is an open-source framework for machine learning **model serving**, aiming to **bridge the gap between Data Science and DevOps**.\n",
    "\n",
    "Data Scientists can easily package their models trained with any ML framework using BentoMl and reproduce the model for serving in production. BentoML helps with managing packaged models in the BentoML format, and allows DevOps to deploy them as online API serving endpoints or offline batch inference jobs, on any cloud platform.\n",
    "\n",
    "This getting started guide demonstrates how to use BentoML to serve a sklearn modeld via a REST API server, and then containerize the model server for production deployment.\n",
    "\n",
    "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=guides&ea=bentoml-quick-start-guide&dt=bentoml-quick-start-guide)"
   ]
  },
  {
   "source": [
    "# Install latest version of Zeblok CLI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "!npm i -g zbl-cli"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, let's prepare a trained model for serving with BentoML. Train a classifier model on the [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Prediction Service with BentoML\n",
    "\n",
    "Model serving with BentoML comes after a model is trained. The first step is creating a\n",
    "prediction service class, which defines the models required and the inference APIs which\n",
    "contains the serving logic. Here is a minimal prediction service created for serving\n",
    "the iris classifier model trained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris_classifier.py\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import DataframeInput\n",
    "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
    "\n",
    "@env(infer_pip_packages=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "    \"\"\"\n",
    "    A minimum prediction service exposing a Scikit-learn model\n",
    "    \"\"\"\n",
    "\n",
    "    @api(input=DataframeInput(), batch=True)\n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        An inference API named `predict` with Dataframe input adapter, which codifies\n",
    "        how HTTP requests or CSV files are converted to a pandas Dataframe object as the\n",
    "        inference API function input\n",
    "        \"\"\"\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a prediction service that packages a scikit-learn model and provides\n",
    "an inference API that expects a `pandas.Dataframe` object as its input. BentoML also supports other API input \n",
    "data types including `JsonInput`, `ImageInput`, `FileInput` and \n",
    "[more](https://docs.bentoml.org/en/latest/api/adapters.html).\n",
    "\n",
    "\n",
    "In BentoML, **all inference APIs are suppose to accept a list of inputs and return a \n",
    "list of results**. In the case of `DataframeInput`, each row of the dataframe is mapping\n",
    "to one prediction request received from the client. BentoML will convert HTTP JSON \n",
    "requests into :code:`pandas.DataFrame` object before passing it to the user-defined \n",
    "inference API function.\n",
    " \n",
    "This design allows BentoML to group API requests into small batches while serving online\n",
    "traffic. Comparing to a regular flask or FastAPI based model server, this can increases\n",
    "the overall throughput of the API server by 10-100x depending on the workload.\n",
    "\n",
    "The following code packages the trained model with the prediction service class\n",
    "`IrisClassifier` defined above, and then saves the IrisClassifier instance to disk \n",
    "in the BentoML format for distribution and deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the IrisClassifier class defined above\n",
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', clf)\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML stores all packaged model files under the\n",
    "`~/bentoml/{service_name}/{service_version}` directory by default.\n",
    "The BentoML file format contains all the code, files, and configs required to \n",
    "deploy the model for serving.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0e5bdba2814bfe1c7d3bc79931d098502bd49e97ae2c2a61adc084e706ae96c76",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}